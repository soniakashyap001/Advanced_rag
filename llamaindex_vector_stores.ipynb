{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9MrU1VhgZq7"
   },
   "source": [
    "# Introduction to Advanced RAG in LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aqG2gONM1TIn",
    "outputId": "cd6ef64d-8fcc-4c11-f435-bed43a4760d7"
   },
   "outputs": [],
   "source": [
    "%pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZnZasH81VBe"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNITVh6fcZRH"
   },
   "outputs": [],
   "source": [
    "%pip install -Uq llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtblxeGJgfqu"
   },
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ew6PmzeJcgZ-"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "docs = SimpleDirectoryReader(input_dir=\"./data\").load_data()\n",
    "\n",
    "# file name as id\n",
    "# docs_nam_as_id = SimpleDirectoryReader(input_dir=\"./data\", filename_as_id=True).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N6_Qs8oFd5U_",
    "outputId": "b1d415e3-f362-488b-896b-c916f5e9d64d"
   },
   "outputs": [],
   "source": [
    "len(docs)  # one per page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kr3oxI_1burS",
    "outputId": "5e1fd25a-96c6-469e-85d7-0d96737fd24c"
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWVwkooyiFMF"
   },
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axrU95QwntZH",
    "outputId": "44da32e9-a225-45c0-ad4c-d4dc13712fbc"
   },
   "outputs": [],
   "source": [
    "# hide some keys from llm\n",
    "\n",
    "docs[0].__dict__ # too much data about one doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hmbdVZ_mpvcH",
    "outputId": "1c25aac8-ec4e-4ecb-f538-a015291246ef"
   },
   "outputs": [],
   "source": [
    "# quick example of what the LLM and Embeddings see when with a test document\n",
    "\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "document = Document(\n",
    "    text=\"This is a super-customized document\",\n",
    "    metadata={\n",
    "        \"file_name\": \"super_secret_document.txt\",\n",
    "        \"category\": \"finance\",\n",
    "        \"author\": \"LlamaIndex\",\n",
    "    },\n",
    "    # excluded_embed_metadata_keys=[\"file_name\"],\n",
    "    excluded_llm_metadata_keys=[\"category\"],\n",
    "    metadata_seperator=\"\\n\",\n",
    "    metadata_template=\"{key}:{value}\",\n",
    "    text_template=\"Metadata:\\n{metadata_str}\\n-----\\nContent:\\n{content}\",\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The LLM sees this: \\n\",\n",
    "    document.get_content(metadata_mode=MetadataMode.LLM),\n",
    ")\n",
    "# print(\n",
    "#     \"The Embedding model sees this: \\n\",\n",
    "#     document.get_content(metadata_mode=MetadataMode.EMBED),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpfv9Auvn_1q",
    "outputId": "12c9eec6-dacd-4b26-bad0-54b81a510996"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "# print(docs[0].get_content(metadata_mode=MetadataMode.LLM))   # what the llm sees\n",
    "print(docs[0].get_content(metadata_mode=MetadataMode.EMBED)) # what embeddings see. in this case, same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bmx1ZCisotj"
   },
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    # define the content/metadata template\n",
    "    doc.text_template = \"Metadata:\\n{metadata_str}\\n---\\nContent:\\n{content}\"\n",
    "\n",
    "    # exclude page label from embedding\n",
    "    if \"page_label\" not in doc.excluded_embed_metadata_keys:\n",
    "        doc.excluded_embed_metadata_keys.append(\"page_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n8hDb9WztOHm",
    "outputId": "ec7ca63a-7887-4625-e5c0-9193dc7d1489"
   },
   "outputs": [],
   "source": [
    "# after editing the content seen by embedings\n",
    "\n",
    "print(docs[0].get_content(metadata_mode=MetadataMode.EMBED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iflpVVGqzN4U"
   },
   "source": [
    "Here are other, more advanced transformations. Some require an LLM to work. We will use Qwen 2.5 32B Instruct 128k through Groq, which is an affordble, high-rate model. It should be enough to extract Q&As and titles from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQtB4HmPzXNG"
   },
   "outputs": [],
   "source": [
    "%pip install -Uq llama-index-llms-groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T2msOttR0OaE",
    "outputId": "21bf334c-10f2-43f9-ed5f-0190566c6c4f"
   },
   "source": [
    "from llama_index.llms.groq import Groq\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z85b4STP7eCU"
   },
   "source": [
    "#llm_transformations = Groq(model=\"qwen-2.5-32b\", api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "from llama_index.llms.groq import Groq\n",
    "import os\n",
    "\n",
    "# transformations: cheap/fast\n",
    "llm_transformations = Groq(\n",
    "    model=\"llama-3.1-8b-instant\",           # ← was \"qwen-2.5-32b\"\n",
    "    api_key=os.environ[\"GROQ_API_KEY\"]\n",
    ")\n",
    "\n",
    "# querying: quality\n",
    "llm_querying = Groq(\n",
    "    model=\"llama-3.3-70b-versatile\",        # you already used this; it's supported\n",
    "    api_key=os.environ[\"GROQ_API_KEY\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U llama-index-llms-openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"your key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Fast + cheaper for transformations (titles, QA)\n",
    "llm_transformations = OpenAI(\n",
    "    model=\"gpt-4o-mini\",           # fast/affordable\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "# Higher quality for answering\n",
    "llm_querying = OpenAI(\n",
    "    model=\"gpt-4.1\",               # strong general model\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396,
     "referenced_widgets": [
      "50090a1a905847e4a32e1ee94b387005",
      "024e8240fff246c6af50b372b175d481",
      "7ca34bb8a6174036b8150363a68ba733",
      "aaf7609e5f3044fcab76e90fad6e1dbe",
      "6f7fac2c8b764e46a25393d29843d807",
      "c02e809e3725456b85d46cbddee0956c",
      "ffac10754aa945ffa983c5e89eb45337",
      "bb823eb791964f2ca72274df6c16d909",
      "cad42c49057447efbd09d5635048d65c",
      "d323d3db33e6400b97604169d9bd3702",
      "43a392b2d250478db39758426e57ba7e"
     ]
    },
    "id": "Xtz9ymYyy0T3",
    "outputId": "37b69a5c-181e-4b38-ad27-c0213b18ae59"
   },
   "outputs": [],
   "source": [
    "# other transformations\n",
    "\n",
    "from llama_index.core.extractors import (\n",
    "    TitleExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "from llama_index.core.extractors import TitleExtractor, QuestionsAnsweredExtractor\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "text_splitter = SentenceSplitter(separator=\" \", chunk_size=1024, chunk_overlap=128)\n",
    "\n",
    "title_extractor = TitleExtractor(llm=llm_transformations, nodes=5)\n",
    "qa_extractor    = QuestionsAnsweredExtractor(llm=llm_transformations, questions=3)\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[text_splitter, title_extractor, qa_extractor]\n",
    ")\n",
    "\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        text_splitter,\n",
    "        title_extractor,\n",
    "        qa_extractor\n",
    "    ]\n",
    ")\n",
    "\n",
    "nodes = pipeline.run(\n",
    "    documents=docs,\n",
    "    in_place=True,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq9Bl6sk28F9"
   },
   "source": [
    "By default, Llamaindex uses OpenAI's embedding models. But you can choose to load a free model from HuggingFace too (but it it will be slower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SHmUuMZ79Qzo",
    "outputId": "bbccab6a-d2df-4bc7-e92e-44a122857a02"
   },
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AWBHMPxy9SzK",
    "outputId": "1978bc16-8973-4cdc-d198-23ef1ff2ae83"
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# pprint.pprint(nodes[0].__dict__)\n",
    "\n",
    "print(nodes[0].get_content(metadata_mode=MetadataMode.LLM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQMILxH8_VJl"
   },
   "source": [
    "## Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q492Kpcaj8GR",
    "outputId": "dfaa9936-8522-4326-a29c-93b0978207d2"
   },
   "outputs": [],
   "source": [
    "%pip install -Uq llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U llama-index-embeddings-openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 511,
     "referenced_widgets": [
      "6487c86402404a55b65c0c2300d23e90",
      "4f0cc711125d47639ccb5097275d9ddb",
      "90b0e9f87f844241a2bd71ba70649d29",
      "91c9451d22e14c0d8c850cbce10ca4f8",
      "2048854a89c54fb7a4ffff3c2088c93c",
      "cb47f2adc1e3408ea3ea20838110fe7d",
      "530ba97fa1fc4caf9c28a0edeb81fbd6",
      "bbecb7a0f9254d7f950d87e0fb9f7213",
      "32dce4f77ede4c3a8bf6d7c48edac0ad",
      "bb032fab0d8c41809fa277a2d1459ac2",
      "fa123fd9fba34c279c7a612b3cfad602",
      "585f8c0f87fe47d3a6eb3c750d6567ec",
      "0effdaee342b4871abb50088f14c6a31",
      "8c1feecd99774a0eb65e9f009fc8e907",
      "097979c786f3403280c673e880d7040a",
      "585dca54f17f43468b2899f8bf53cbd7",
      "fd769a014cd647348fd998fd01de1456",
      "edb9877a50434060864202fcf91862e4",
      "bf84a30624be445bb594d08415964411",
      "87a8a98697c74d8babe16deb6861e8be",
      "63a251739ae44a94b43ca7120f9ca506",
      "3def064ddadf4d96834426d328775139",
      "54c144fda9b44182ae52fff76c703f32",
      "7919b7d33e8346269fa9ca36b4027ed4",
      "f99f756949984059af5a56867073cdef",
      "dcfc01aba66c46cb9aa7a805485c2676",
      "a3556f7158084884a6c59b33bdf750b7",
      "87ad42306bf94ccc89e4ad052a3d6d57",
      "f14b4da1bdc34b87be985abf78530f7a",
      "a8b05376e7f94bffaa313dd526fef46d",
      "7e6c754afe4c4b658b114ac04b659d06",
      "a72fd6ce2d924d12911a812d8f845eef",
      "3d260ecd240544aa94fd90eded12c660",
      "52907b61e1b041db88a74c8a39d88ca5",
      "42827a5c847c4f60a002b27bcb0c82a9",
      "ee37016a0afa4dafb58563c39a5c5dc2",
      "78812a7bfdfb431489f9f73389932cb6",
      "d4895c0aa7e34d92bc73e640411b8359",
      "a0b578c213df467b981572be91af1eac",
      "525e7fed7c7341fe92527610b10e6257",
      "2c14adf4d49848bc8ac702fc6c86e9a5",
      "053e521768ee4c6cbbc7df1b7d6c35d3",
      "e2d1b57fa636448e9b066e3341ba4958",
      "d528bb4242784a019d66ccd22744e7e1",
      "b0b89fa2b3f040e481493959808f63a4",
      "9daaa444e93c4dda911a79bf6158e401",
      "102bfa341d0041a09aace6a9a1ad6927",
      "94af3670682749a5955fb63eab6b0a80",
      "19e3717c839941129c847079fb85587a",
      "f333896e5f7e404b8d2090f2d1896857",
      "1c6ea5761541414ba2648913c9f68591",
      "04f40645b35740449376e1f5cde65655",
      "38b1bc8a8fa445ff8d38aeff74007294",
      "0540b71ea39d41278b3e19e2b2479f2a",
      "d2bd5c77415e40f39673a8d713916b25",
      "cc3292b9c7d74e8588e3dfb0105d2695",
      "c0e1958aa62448a99d464622743c381c",
      "aa9ef684b2b444b9be45eeeacff70e8f",
      "78fb05bf5a0b4ca3aaaa5b06da0659b7",
      "2643a589bed14e0998b7dabef4f5ed94",
      "b09473fa8eee42338ca04d37215be237",
      "f1e5880d8e204ea0b24bcabcb12fca64",
      "8f032fdb793f46749dce7190fb32afe9",
      "f56f29eaa33d4270b88a40d8fae5262d",
      "862455d1cd7f486b8e5d63471022cd5b",
      "1e3d0cca925b4e0f9bc7cc6e267713dd",
      "af14f159fee14c63886c824e9e13d9de",
      "b66ab1a9d5424b02a333062249287183",
      "c3e41cd749564462add3634d87b80101",
      "e26f6f229c0c4bb6be571a989b5c925a",
      "33869482cc494ba6ad61ab8acef98fa3",
      "4b4f2b94cf7a449f87d7e89ef022ea76",
      "ed2b6611e381425eaf66ee91435c5b4a",
      "3ab82be85e084df89f835bd6c878c496",
      "f9caae9bdc41400982d4d9db05990d7c",
      "02c9bc1b82254197b13c480a8b501b1c",
      "94d6dfcc9c6b4837ac53fc26c8ba3501",
      "0bac2207e58b451886b4392d2df2d605",
      "fda8960bbcb74542b42f9f6a5cdce908",
      "d46ae126e36847acb2a5c59c9e63ac2b",
      "8315ec95bf0143a6a6d9c2d7f1c752c4",
      "3ed4884f50594ac5b218b3def9154996",
      "331d0b5e1964461aa7cf518d53324230",
      "083b917f31364466a1ad859cd42b098a",
      "1a2c13d0b9fd4425bce6bbc7c589f221",
      "f072c6685f8046fc8df3fea11b471b75",
      "684a7baac1904888aac73f5c627b1193",
      "a48a98e17ce64b2681b542b1740de270",
      "b3d69e5a99114c618b6a124e41c1ad0f",
      "3483b19492e447b9b64dbc7a07b5c58d",
      "5ca994e4684d4e32b0548f3d402e0679",
      "d21de1bb5a584b1d83aeac4af6b3d6a4",
      "0cca6ca326434305919d1717799f4e7a",
      "d2634d4a8ec649b6af98f0095a69865f",
      "77ad6a3030694bd79778c286826e77fa",
      "ece3bf82fc4f4d18abecd60268808c56",
      "b3bb7903cd344bcab50747840e4e3043",
      "5282e8fd1a414f8eb588dc87163d716e",
      "6cf4367a56754124b4963d51f8cb4159",
      "de522a701f104625a2d7f55cf40c473e",
      "f3c8d1aa7fa94e9ebd9b45167bce063a",
      "84d26ce841754159a704ac2c2c5e5160",
      "6fb2fd40f99d4f8ea02107a6216de139",
      "3776261b4cdb4ffbbf68d404f8bc0ff8",
      "2c5d1e19a8a14bc4bbb833ecc558c6f7",
      "300cec9a848b4c29bc5320db8bd433d4",
      "699655708fb74eef96c376661a1e3353",
      "be20848f4266409e9c52fe4df00c1cfc",
      "6e6215f9695a4f3c8a0b2ec55b5d5e14",
      "01beea19411141238a63870b26f5879c",
      "0822f2758e44404186c0fe0f65f5b7b3",
      "55c2515d42424875a808f20acf372620",
      "c2106309ac66405f9ba758730451477f",
      "dd70d845be0b4f3888513cec6d48a390",
      "12154920aa1f4358a54cc93ef8d76d6a",
      "cbe1643c6ab047b5b3ff882a19890553",
      "e6edcb2f263741bba36c63b91c281d72",
      "75c3acc0c14642e88d04eb5ff5014954",
      "45d4a79a58554a62a64bb2d8df9dc888",
      "2500cdff07fe40d9a1e47fd7081d4297",
      "a31a75ba72e94b4895d64ce53ad4e6b2"
     ]
    },
    "id": "gWLHSg0Re_Xr",
    "outputId": "4e0c2143-bfd7-4cff-f14b-b6c2453053f4"
   },
   "source": [
    "# Embeddings\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "test_embed = hf_embeddings.get_text_embedding(\"Hello world\")\n",
    "print(test_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")  # great quality/cost\n",
    "# test:\n",
    "print(len(embed_model.get_text_embedding(\"hello\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Ffev4Gj9tA2"
   },
   "outputs": [],
   "source": [
    "# create index\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "source_objects = nodes if \"nodes\" in locals() else docs\n",
    "index = VectorStoreIndex(source_objects, embed_model=embed_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbXqTcgH-LPb"
   },
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sLuIYx5Q-K2T",
    "outputId": "2064c3be-b7cb-425a-97cc-461877933d15"
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# define your answering llm\n",
    "llm_querying = OpenAI(\n",
    "    model=\"gpt-4o-mini\",               # fast + cost-effective\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "# build the query engine with that llm\n",
    "query_engine = index.as_query_engine(llm=llm_querying)\n",
    "\n",
    "# run a sample query\n",
    "response = query_engine.query(\"what does this model do?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DXXv87vl-2PQ",
    "outputId": "826651fc-1969-4bea-bbe0-0e4d5e51b7b3"
   },
   "outputs": [],
   "source": [
    "response.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLXyCIIZ_crt"
   },
   "source": [
    "## Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSvxtaGC_eRs"
   },
   "outputs": [],
   "source": [
    "index.storage_context.persist(persist_dir=\"./vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Vq6DhIU_0GZ"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "# rebuild storage context from your persisted folder\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./vectors\")\n",
    "\n",
    "# load index with your OpenAI embed model\n",
    "index_from_storage = load_index_from_storage(storage_context, embed_model=embed_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6UPLaRvE-5t"
   },
   "outputs": [],
   "source": [
    "qa = index_from_storage.as_query_engine(llm=llm_querying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8caGYDSyFPbr",
    "outputId": "2d55f94f-8ba1-42fe-c3e4-41ccae078d23"
   },
   "outputs": [],
   "source": [
    "response = qa.query(\"what does this model do?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXSPYn9IFZ_U"
   },
   "source": [
    "# Using Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DUNlAGCUFcSi",
    "outputId": "11e3e6c0-c4f6-4f6f-bce7-d16b53d7a3ab"
   },
   "outputs": [],
   "source": [
    "%pip install -Uq chromadb\n",
    "%pip install -Uq llama-index-vector-stores-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eo4G5gyGHuBK"
   },
   "outputs": [],
   "source": [
    "# REQUIRES:\n",
    "# pip install chromadb llama-index-vector-stores-chroma\n",
    "\n",
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "# 1) Chroma persistent client & collection\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"healthGPT\")\n",
    "\n",
    "# 2) Wire vector store to storage context\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# 3) Build index from your prepared nodes (use OpenAI embeddings)\n",
    "index = VectorStoreIndex(\n",
    "    nodes,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,        # ← was hf_embeddings\n",
    ")\n",
    "\n",
    "# 4) Query\n",
    "query_engine = index.as_query_engine(llm=llm_querying)\n",
    "response = query_engine.query(\"What is this model good at?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KR6mFCDaIdAM",
    "outputId": "a7492e86-d848-4149-fc3a-4154c3aee1b7"
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is this model good at?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coarse recall from the vector store\n",
    "retriever = index.as_retriever(similarity_top_k=20)  # try 20–30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import LLMRerank\n",
    "\n",
    "# Keep only the best M chunks after reranking\n",
    "reranker = LLMRerank(top_n=5, llm=llm_querying)  # M=5 is a good start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import LLMRerank\n",
    "\n",
    "# fine filter: keep best 5 after rerank\n",
    "reranker = LLMRerank(top_n=5, llm=llm_querying)\n",
    "\n",
    "# build query engine:\n",
    "# - similarity_top_k=20 sets the coarse ANN recall\n",
    "# - node_postprocessors applies the reranker\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=llm_querying,\n",
    "    similarity_top_k=20,\n",
    "    node_postprocessors=[reranker],\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"what is this model good at?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"what is this model good at?\"\n",
    "cands = index.as_retriever(similarity_top_k=20).retrieve(q)\n",
    "reranked = LLMRerank(top_n=5, llm=llm_querying).postprocess_nodes(cands, query_str=q)\n",
    "\n",
    "for i, n in enumerate(reranked, 1):\n",
    "    print(f\"{i:02d}. score={getattr(n, 'score', None)} | source={n.metadata.get('file_name')}\")\n",
    "    print(n.get_content()[:220].replace(\"\\n\",\" \"), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek at what the extractors wrote into metadata so we know the keys\n",
    "n0 = nodes[0]\n",
    "print(\"Known keys on node[0].metadata:\", list(n0.metadata.keys()))\n",
    "print(\"\\nTitle (if present):\", n0.metadata.get(\"title\") or n0.metadata.get(\"document_title\"))\n",
    "print(\"\\nQuestions (if present):\", n0.metadata.get(\"questions\") or n0.metadata.get(\"questions_this_node_can_answer\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: inspect QA metadata on your nodes\n",
    "\n",
    "def get_qas(md):\n",
    "    # try common keys your pipeline may use\n",
    "    for key in (\n",
    "        \"questions_this_excerpt_can_answer\",\n",
    "        \"questions_this_node_can_answer\",\n",
    "        \"questions\",\n",
    "        \"qa_pairs\",\n",
    "    ):\n",
    "        if key in md and md[key]:\n",
    "            qas = md[key]\n",
    "            # normalize to list[str]\n",
    "            if isinstance(qas, str):\n",
    "                return [qas]\n",
    "            try:\n",
    "                return [str(q) for q in qas]\n",
    "            except Exception:\n",
    "                return [str(qas)]\n",
    "    return []\n",
    "\n",
    "total = len(nodes)\n",
    "with_qas = 0\n",
    "samples = []\n",
    "\n",
    "for n in nodes:\n",
    "    qas = get_qas(n.metadata or {})\n",
    "    if qas:\n",
    "        with_qas += 1\n",
    "        if len(samples) < 3:\n",
    "            samples.append({\n",
    "                \"title\": n.metadata.get(\"title\") or n.metadata.get(\"document_title\"),\n",
    "                \"qas\": qas[:5],\n",
    "                \"text_preview\": n.text[:200].replace(\"\\n\",\" \")\n",
    "            })\n",
    "\n",
    "print(f\"Total nodes: {total}\")\n",
    "print(f\"Nodes that have non-empty QA metadata: {with_qas}\")\n",
    "\n",
    "for i, s in enumerate(samples, 1):\n",
    "    print(f\"\\n--- Sample node {i} ---\")\n",
    "    print(\"Title:\", s[\"title\"])\n",
    "    print(\"QAs:\", s[\"qas\"])\n",
    "    print(\"Text preview:\", s[\"text_preview\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Sequence, Any\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "def _coerce_list(x: Any) -> List[str]:\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return [str(i) for i in x]\n",
    "    return [str(x)]\n",
    "\n",
    "def _strip_outer_quotes(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    if (s.startswith('\"') and s.endswith('\"')) or (s.startswith(\"'\") and s.endswith(\"'\")):\n",
    "        return s[1:-1]\n",
    "    return s\n",
    "\n",
    "def _extract_questions(q_blob: Any) -> List[str]:\n",
    "    \"\"\"\n",
    "    Your QA extractor is storing a long Markdown-like block as a single string.\n",
    "    This pulls out clean question lines so we don't embed the whole paragraph.\n",
    "    \"\"\"\n",
    "    text = \"\\n\".join(_coerce_list(q_blob))\n",
    "    # normalize bullets/markdown\n",
    "    text = text.replace(\"**\", \"\")\n",
    "    # regex: grab sentences that look like questions (end with '?')\n",
    "    candidates = re.findall(r'([^?]{3,}?\\?)', text, flags=re.MULTILINE|re.DOTALL)\n",
    "    # strip numbering like \"1. \" or \"- \"\n",
    "    cleaned = []\n",
    "    for c in candidates:\n",
    "        line = re.sub(r'^\\s*(\\d+[\\.\\)]\\s+|\\-\\s+|\\*\\s+)?', '', c.strip())\n",
    "        # keep reasonably sized questions\n",
    "        if 5 <= len(line) <= 220:\n",
    "            cleaned.append(line)\n",
    "    # dedupe while preserving order\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for q in cleaned:\n",
    "        k = q.lower()\n",
    "        if k not in seen:\n",
    "            seen.add(k)\n",
    "            unique.append(q)\n",
    "    # cap to a handful so embedding text stays compact\n",
    "    return unique[:5]\n",
    "\n",
    "def make_nodes_with_title_and_qas(nodes: Sequence[TextNode]) -> List[TextNode]:\n",
    "    out = []\n",
    "    for n in nodes:\n",
    "        md = n.metadata or {}\n",
    "        title = md.get(\"title\") or md.get(\"document_title\") or \"\"\n",
    "        title = _strip_outer_quotes(title) if isinstance(title, str) else title\n",
    "        q_blob = (\n",
    "            md.get(\"questions_this_excerpt_can_answer\")\n",
    "            or md.get(\"questions_this_node_can_answer\")\n",
    "            or md.get(\"questions\")\n",
    "            or md.get(\"qa_pairs\")\n",
    "        )\n",
    "        q_list = _extract_questions(q_blob)\n",
    "\n",
    "        parts = []\n",
    "        if title:\n",
    "            parts.append(f\"[Title] {title}\")\n",
    "        parts.append(n.text)  # original chunk\n",
    "        if q_list:\n",
    "            parts.append(\"[QuestionsThisChunkCanAnswer]\")\n",
    "            parts.extend(f\"- {q}\" for q in q_list)\n",
    "\n",
    "        enriched_text = \"\\n\".join(parts)\n",
    "\n",
    "        out.append(TextNode(\n",
    "            id_=n.node_id,\n",
    "            text=enriched_text,\n",
    "            metadata=dict(md),\n",
    "        ))\n",
    "    return out\n",
    "\n",
    "# Build enriched nodes and preview one\n",
    "nodes_with_title_qas = make_nodes_with_title_and_qas(nodes)\n",
    "print(\"✅ Built enriched nodes with title + parsed QAs folded into text.\")\n",
    "print(\"\\nPreview enriched text:\\n\", nodes_with_title_qas[0].text[:800])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Build a new index so embeddings include [Title] + parsed QAs\n",
    "index_with_meta = VectorStoreIndex(\n",
    "    nodes_with_title_qas,\n",
    "    embed_model=embed_model  # your OpenAI embedding model from earlier\n",
    ")\n",
    "\n",
    "print(\"✅ Index rebuilt with metadata-aware embeddings (index_with_meta).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build query engine (you can also plug in reranker later)\n",
    "query_engine = index_with_meta.as_query_engine(llm=llm_querying)\n",
    "\n",
    "# test with a question that overlaps the QAs we injected\n",
    "response = query_engine.query(\n",
    "    \"What novel technique does HealthGPT employ to adapt heterogeneous knowledge?\"\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = query_engine.query(\n",
    "    \"What novel technique does HealthGPT employ to adapt heterogeneous knowledge?\"\n",
    ")\n",
    "\n",
    "print(\"\\nANSWER:\\n\", resp)\n",
    "\n",
    "print(\"\\n--- SOURCES USED ---\")\n",
    "for i, s in enumerate(resp.source_nodes, 1):\n",
    "    md = s.node.metadata or {}\n",
    "    title = md.get(\"title\") or md.get(\"document_title\") or \"(untitled)\"\n",
    "    page  = md.get(\"page_label\") or \"?\"\n",
    "    fname = md.get(\"file_name\") or \"?\"\n",
    "    print(f\"\\n{i}. score={getattr(s, 'score', None)}\")\n",
    "    print(f\"   Title: {title}\")\n",
    "    print(f\"   File/Page: {fname} / {page}\")\n",
    "    print(\"   Snippet:\", s.node.get_content()[:300].replace(\"\\n\",\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import LLMRerank\n",
    "\n",
    "# Keep best 5 after LLM-based reranking\n",
    "reranker = LLMRerank(top_n=5, llm=llm_querying)\n",
    "\n",
    "# Build a query engine that:\n",
    "# - retrieves top-20 by embeddings (from your metadata-enriched index)\n",
    "# - reranks them down to top-5\n",
    "query_engine = index_with_meta.as_query_engine(\n",
    "    llm=llm_querying,\n",
    "    similarity_top_k=20,\n",
    "    node_postprocessors=[reranker],\n",
    ")\n",
    "\n",
    "resp = query_engine.query(\"What novel technique does HealthGPT employ to adapt heterogeneous knowledge?\")\n",
    "print(resp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step — Inspect rerank vs. embedding similarity scores\n",
    "\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "\n",
    "# 1) pick your question (edit as you like)\n",
    "q = \"What novel technique does HealthGPT employ to adapt heterogeneous knowledge?\"\n",
    "\n",
    "# 2) retrieve top-K by embeddings (coarse recall)\n",
    "K = 20\n",
    "retriever = index_with_meta.as_retriever(similarity_top_k=K)\n",
    "candidates = retriever.retrieve(q)\n",
    "\n",
    "# keep original (embedding) scores by node_id\n",
    "orig_scores = {nws.node.node_id: (nws.score or 0.0) for nws in candidates}\n",
    "\n",
    "# 3) rerank down to top-M using the answering LLM (fine precision)\n",
    "M = 5\n",
    "reranker = LLMRerank(top_n=M, llm=llm_querying)\n",
    "reranked = reranker.postprocess_nodes(candidates, query_str=q)\n",
    "\n",
    "# 4) pretty print comparison\n",
    "print(f\"Query: {q}\\n\")\n",
    "print(f\"Embedding-retrieved K={K}, LLM-reranked M={M}\\n\")\n",
    "\n",
    "for i, nws in enumerate(reranked, 1):\n",
    "    rerank_score = nws.score  # LLM-based relevance score\n",
    "    node = nws.node\n",
    "    md = node.metadata or {}\n",
    "    title = md.get(\"title\") or md.get(\"document_title\") or \"(untitled)\"\n",
    "    page  = md.get(\"page_label\") or \"?\"\n",
    "    fname = md.get(\"file_name\") or \"?\"\n",
    "    # original embedding similarity (from the first retrieval)\n",
    "    emb_score = orig_scores.get(node.node_id, None)\n",
    "\n",
    "    print(f\"{i}. RERANK={rerank_score:.3f} | EMBEDDING={emb_score:.3f}  | {title}\")\n",
    "    print(f\"   Source: {fname} / p.{page}\")\n",
    "    print(\"   Snippet:\", node.get_content()[:220].replace(\"\\n\", \" \"))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LLM reranker inside the query engine so only top reranked chunks feed the LLM\n",
    "\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "\n",
    "K = 20  # coarse recall (embedding retrieval)\n",
    "M = 5   # fine precision (reranker keeps best M)\n",
    "\n",
    "reranker = LLMRerank(top_n=M, llm=llm_querying)\n",
    "\n",
    "# Build a query engine that:\n",
    "#  - retrieves top-K by embeddings from your metadata-enriched index\n",
    "#  - reranks to top-M with the LLM\n",
    "#  - passes only those M chunks to the answering LLM\n",
    "qe_reranked = index_with_meta.as_query_engine(\n",
    "    llm=llm_querying,\n",
    "    similarity_top_k=K,\n",
    "    node_postprocessors=[reranker],\n",
    ")\n",
    "\n",
    "# Try it\n",
    "resp = qe_reranked.query(\"What novel technique does HealthGPT employ to adapt heterogeneous knowledge?\")\n",
    "print(\"ANSWER:\\n\", resp)\n",
    "\n",
    "print(\"\\n--- RERANKED SOURCES ---\")\n",
    "for i, s in enumerate(resp.source_nodes, 1):\n",
    "    md = s.node.metadata or {}\n",
    "    title = md.get(\"title\") or md.get(\"document_title\") or \"(untitled)\"\n",
    "    page  = md.get(\"page_label\") or \"?\"\n",
    "    fname = md.get(\"file_name\") or \"?\"\n",
    "    print(f\"{i}. score={getattr(s, 'score', None)} | {title} — {fname} / p.{page}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "\n",
    "QUESTION = \"What novel technique does HealthGPT employ to adapt heterogeneous knowledge?\"\n",
    "\n",
    "# ---------- Baseline: embeddings only ----------\n",
    "t0 = time.time()\n",
    "qe_base = index_with_meta.as_query_engine(\n",
    "    llm=llm_querying,\n",
    "    similarity_top_k=5,          # directly take top-5 by embeddings\n",
    "    node_postprocessors=[],      # no rerank\n",
    ")\n",
    "resp_base = qe_base.query(QUESTION)\n",
    "t_base = time.time() - t0\n",
    "\n",
    "# ---------- Reranked: embeddings (K=20) + LLM rerank (M=5) ----------\n",
    "t1 = time.time()\n",
    "reranker = LLMRerank(top_n=5, llm=llm_querying)\n",
    "qe_rerank = index_with_meta.as_query_engine(\n",
    "    llm=llm_querying,\n",
    "    similarity_top_k=20,         # K\n",
    "    node_postprocessors=[reranker],  # -> M=5\n",
    ")\n",
    "resp_rerank = qe_rerank.query(QUESTION)\n",
    "t_rerank = time.time() - t1\n",
    "\n",
    "# ---------- Pretty print ----------\n",
    "def print_sources(resp, title):\n",
    "    print(f\"\\n--- {title} SOURCES ---\")\n",
    "    for i, s in enumerate(resp.source_nodes, 1):\n",
    "        md = s.node.metadata or {}\n",
    "        title = md.get(\"title\") or md.get(\"document_title\") or \"(untitled)\"\n",
    "        page  = md.get(\"page_label\") or \"?\"\n",
    "        fname = md.get(\"file_name\") or \"?\"\n",
    "        print(f\"{i}. score={getattr(s, 'score', None)} | {title} — {fname} / p.{page}\")\n",
    "\n",
    "print(\"\\n==================== BASELINE (Embeddings only) ====================\")\n",
    "print(f\"Time: {t_base:.2f}s\")\n",
    "print(\"ANSWER:\\n\", resp_base)\n",
    "print_sources(resp_base, \"BASELINE\")\n",
    "\n",
    "print(\"\\n==================== RERANKED (Embeddings + LLM Rerank) ====================\")\n",
    "print(f\"Time: {t_rerank:.2f}s\")\n",
    "print(\"ANSWER:\\n\", resp_rerank)\n",
    "print_sources(resp_rerank, \"RERANKED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "advancedrag (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
